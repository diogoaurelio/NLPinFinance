{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to calculate\n",
    "1. [Flesch–Kincaid readability Grade Index](https://en.wikipedia.org/wiki/Flesch–Kincaid_readability_tests)  \n",
    "2. [Gunning Fog Index](https://en.wikipedia.org/wiki/Gunning_fog_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import os.path\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(cik, priorto, count):\n",
    "    link = \"http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\"+ \\\n",
    "        str(cik)+\"&type=10-K&dateb=\"+str(priorto)+\\\n",
    "        \"&owner=exclude&output=xml&count=\"+str(count)\n",
    "    \"\"\"\n",
    "     given cik and priorto ('date'), count parameters, the functions gets \n",
    "     all the links on the search page on SEC Edgar database\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the website and extract links\n",
    "    data = requests.get(link).text\n",
    "    # print(\"see tentative links for all documents:\")\n",
    "    # print(link)\n",
    "    \n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    # store the link in the list\n",
    "    links = []\n",
    "\n",
    "    # If the link is .htm convert it to .html\n",
    "    for link in soup.find_all('filinghref'):\n",
    "\n",
    "        # convert http://*-index.htm to http://*.txt\n",
    "        url = link.string\n",
    "        if link.string.split(\".\")[len(link.string.split(\".\"))-1] == \"htm\":\n",
    "            url += \"l\"\n",
    "        required_url = url.replace('-index.html', '')\n",
    "        txtdoc = required_url + \".txt\"\n",
    "        #docname = txtdoc.split(\"/\")[-1]\n",
    "        links.append(txtdoc)\n",
    "    return links\n",
    "\n",
    "# clean up the soup we construct from the links\n",
    "def clean_soup(link):\n",
    "    data = requests.get(link).text\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    blacklist = [\"script\", \"style\"]\n",
    "    attrlist = [\"class\", \"id\", \"name\", \"style\", 'cellpadding', 'cellspacing']\n",
    "    skiptags = ['font', 'a', 'b', 'i', 'u']\n",
    "    \n",
    "    for tag in soup.findAll():\n",
    "        if tag.name.lower() in blacklist:\n",
    "            # blacklisted tags are removed in their entirety\n",
    "            tag.extract()\n",
    "\n",
    "        if tag.name.lower() in skiptags:\n",
    "            tag.replaceWithChildren()\n",
    "            \n",
    "        for attribute in attrlist:\n",
    "            del tag[attribute]\n",
    "            \n",
    "                    \n",
    "    return soup\n",
    "\n",
    "\n",
    "# normalize the text\n",
    "# remove some escape characters\n",
    "def normtxt(txt):\n",
    "    return unicodedata.normalize(\"NFKD\",txt)\n",
    "\n",
    "# get section from 10K\n",
    "# looks for the term \"item 1a\" and collects text until \"item 1b\" is found\n",
    "# returns None if there is no appropriate section found\n",
    "# raise error when it cannot find the end of the section\n",
    "\n",
    "def extract_section(soup, section='1a', section_end='1b'):\n",
    "    \n",
    "    search_next = [\"p\", \"div\", \"table\"]\n",
    "    \n",
    "    # loop over all tables\n",
    "    items = soup.find_all((\"table\", \"div\"))\n",
    "\n",
    "    myitem = None\n",
    "    \n",
    "    search_txt = ['item '+ section ]\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        txt = normtxt(item.get_text())\n",
    "        \n",
    "        # this is to avoid long sentences or tables that contain the item\n",
    "        if len(txt.split()) > 5:\n",
    "            continue\n",
    "        if any([w in txt.lower() for w in search_txt]):\n",
    "            myitem = item\n",
    "            \n",
    "    if myitem is None:\n",
    "        # print(\"section not found, returned None\")\n",
    "        return None\n",
    "        \n",
    "    lines = \"\"\n",
    "    des = myitem.find_next(search_next)\n",
    "    \n",
    "    search_txt = [ 'item '+section_end ]\n",
    "\n",
    "    while not des is None:\n",
    "        des = des.find_next(search_next)\n",
    "        \n",
    "        if des is None:\n",
    "            raise ValueError(\"end section not properly found\")\n",
    "            \n",
    "        if any([w in normtxt(des.get_text()).lower() for w in search_txt]):\n",
    "            break\n",
    "            \n",
    "        elif des is not None:\n",
    "            if len(des.get_text().split()) > 2 and '|' not in normtxt(des.get_text()):\n",
    "                # need to get rid of escape characters\n",
    "                lines += normtxt(\" \"+des.get_text())\n",
    "            #elif len(des.get_text().split()) > 2:\n",
    "                #print(\"removing text: \",des.get_text())\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return lines[1:]\n",
    "    \n",
    "    \n",
    "\n",
    "def get_files(cik, company, n=5, max_n=20):\n",
    "    mylinks = get_links(cik, '20170601', str(max_n))\n",
    "    dates = range(2017, 1000, -1)\n",
    "    print(\"downloading 10-Ks item 1A for CIK =\",cik, \"...\")\n",
    "    result_txt = []\n",
    "    i=0\n",
    "    for link in mylinks:\n",
    "        filename = company+\"_10k_\"+str(dates[i])+\".txt\"\n",
    "\n",
    "        if os.path.isfile(filename):\n",
    "            print(\"skipping \"+filename)\n",
    "            i+=1\n",
    "            \n",
    "            if i >= n:\n",
    "                break\n",
    "\n",
    "            continue\n",
    "\n",
    "        soup = clean_soup(link)\n",
    "        section = extract_section(soup)\n",
    "        \n",
    "        if section is None:\n",
    "            continue\n",
    "        \n",
    "        print(\"writing \"+os.path.join('Data',filename)+\" in Data folder ...\")\n",
    "        \n",
    "        with open(os.path.join('Data',filename),\"w\") as f:\n",
    "            f.write(section)\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "        if i >= n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 10-Ks item 1A for CIK = 0001065088 ...\n",
      "writing Data/EBAY_10k_2017.txt in Data folder ...\n",
      "writing Data/EBAY_10k_2016.txt in Data folder ...\n",
      "writing Data/EBAY_10k_2015.txt in Data folder ...\n",
      "writing Data/EBAY_10k_2014.txt in Data folder ...\n",
      "writing Data/EBAY_10k_2013.txt in Data folder ...\n",
      "downloading 10-Ks item 1A for CIK = 0000320193 ...\n",
      "writing Data/AAPL_10k_2017.txt in Data folder ...\n",
      "writing Data/AAPL_10k_2016.txt in Data folder ...\n",
      "writing Data/AAPL_10k_2015.txt in Data folder ...\n",
      "writing Data/AAPL_10k_2014.txt in Data folder ...\n",
      "writing Data/AAPL_10k_2013.txt in Data folder ...\n",
      "downloading 10-Ks item 1A for CIK = 0001310067 ...\n",
      "writing Data/SHLDQ_10k_2017.txt in Data folder ...\n",
      "writing Data/SHLDQ_10k_2016.txt in Data folder ...\n",
      "writing Data/SHLDQ_10k_2015.txt in Data folder ...\n",
      "writing Data/SHLDQ_10k_2014.txt in Data folder ...\n",
      "writing Data/SHLDQ_10k_2013.txt in Data folder ...\n"
     ]
    }
   ],
   "source": [
    "CIK = {'ebay': '0001065088', 'apple':'0000320193', 'sears': '0001310067'}\n",
    "get_files(CIK['ebay'], 'EBAY')\n",
    "get_files(CIK['apple'], 'AAPL')\n",
    "get_files(CIK['sears'], 'SHLDQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safeRead(fname):\n",
    "    with open(fname,'r') as f:\n",
    "        text = ''.join(f).replace(';','.')\n",
    "    return text\n",
    "text_phy = safeRead('Data/physics.txt')\n",
    "text_alice = safeRead('Data/alice.txt')\n",
    "text_10k = safeRead('Data/AAPL_10k_2017.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following discussion of risk factors contains forward-looking statements. These risk factors may be important to understanding other statements in this Form 10-K. The following information should be read in conjunction with Part II, Item 7, “Management’s Discussion and Analysis of Financial Condition and Results of Operations” and the consolidated financial statements and related notes in Part II, Item 8, “Financial Statements and Supplementary Data” of this Form 10-K. The business, financia...\n",
      "\n",
      "In particle physics, supersymmetry (SUSY) is a principle that proposes a relationship between two basic classes of elementary particles: bosons, which have an integer-valued spin, and fermions, which have a half-integer spin. A type of spacetime symmetry, supersymmetry is a possible candidate for undiscovered particle physics, and seen as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to...\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice, “without pictures or conversations?”. So she was considering, in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of ge...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_10k[:500]+\"...\\n\")\n",
    "print(text_phy[:500]+\"...\\n\")\n",
    "print(text_alice[:500]+\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/amiteshsinha/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amiteshsinha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/amiteshsinha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cmudict')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cmudict.dict()\n",
    "sno = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def syllable_count(word):\n",
    "    try:\n",
    "        return np.min([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return _syllables(word)\n",
    "\n",
    "def _syllables(word):\n",
    "#referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "# tokenizer that selects out non letter and non symbol (i.e. all alphabets)\n",
    "word_tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    return [ w for w in word_tokenizer.tokenize(sent) if w.isalpha() ]\n",
    "\n",
    "def sentence_count(text):\n",
    "    return len(sent_tokenizer.tokenize(text))\n",
    "\n",
    "def word_count(sent):\n",
    "    return len([ w for w in word_tokenize(sent)])\n",
    "\n",
    "def hard_word_count(sent):\n",
    "    return len([ w for w in word_tokenize(sent) \\\n",
    "                if syllable_count(wnl.lemmatize(w, pos='v'))>=3 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'following',\n",
       " 'discussion',\n",
       " 'of',\n",
       " 'risk',\n",
       " 'factors',\n",
       " 'contains',\n",
       " 'forward',\n",
       " 'looking',\n",
       " 'statements',\n",
       " 'These',\n",
       " 'risk',\n",
       " 'factors',\n",
       " 'may']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text_10k[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Readability Grade-Levels\n",
    "\n",
    "Here, we will implement the two readability indices (grade levels). They are defined by\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Flesch–Kincaid Grade} \n",
    "= 0.39 \\left(\n",
    "\\frac{\\textrm{Number of words}}{\\textrm{Number of sentences}}\n",
    "\\right) \\\\\n",
    "+11.8\n",
    "\\left(\n",
    "\\frac{\\textrm{Number of syllables}}{\\textrm{Number of words}}\n",
    "\\right)\n",
    "-15.59\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Gunning-Fog Grade} \n",
    "=\\; &0.4 \\bigg[ \n",
    "\\left(\n",
    "\\frac{\\textrm{Number of words}}{\\textrm{Number of sentences}}\n",
    "\\right) \n",
    "+100\n",
    "\\left(\n",
    "\\frac{\\textrm{Number of hard words}}{\\textrm{Number of words}}\n",
    "\\right)\n",
    "\\bigg]\n",
    "\\end{align}\n",
    "\n",
    "To count syllables, we've added a syllable_count function you can access via \n",
    "\n",
    "```\n",
    "from syllable_count import syllable_count\n",
    "syllable_count(\"syllable\")\n",
    "```\n",
    "\n",
    "Below, implement the function `flesch_index` and `fog_index` that computes the readability grade level for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch_index(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = np.sum([ word_count(s) for s in sentences ])\n",
    "    total_syllables = np.sum([ np.sum([ syllable_count(w) for w in word_tokenize(s) ]) \\\n",
    "                              for s in sentences ])\n",
    "    \n",
    "    return 0.39*(total_words/total_sentences) + \\\n",
    "            11.8*(total_syllables/total_words) - 15.59\n",
    "\n",
    "def fog_index(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = np.sum([ word_count(s) for s in sentences ])\n",
    "    total_hard_words = np.sum([ hard_word_count(s) for s in sentences ])\n",
    "    \n",
    "    return 0.4*((total_words/total_sentences) + \\\n",
    "            100.0*(total_hard_words/total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.78094652406 9.73654188948\n",
      "16.3171712123 19.3225332001\n",
      "18.2108288106 21.5614490682\n"
     ]
    }
   ],
   "source": [
    "print(flesch_index(text_alice),fog_index(text_alice))\n",
    "print(flesch_index(text_phy),fog_index(text_phy))\n",
    "print(flesch_index(text_10k),fog_index(text_10k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a grade level around 7-10 for `alice.txt`, and around 16-19 for `physics.txt`, and 18+ for financial documents! \n",
    "\n",
    "It turns out 10-Ks are really *hard* to read legal documents!\n",
    "Now, let's compute the readability for all the 10-Ks we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/AAPL_10k_2013.txt 18.1336596757 21.4219541786\n",
      "Data/AAPL_10k_2014.txt 18.1536894665 21.533048686\n",
      "Data/AAPL_10k_2015.txt 18.2144706379 21.6060051245\n",
      "Data/AAPL_10k_2016.txt 18.2620196893 21.6361424013\n",
      "Data/AAPL_10k_2017.txt 18.2108288106 21.5614490682\n",
      "Data/EBAY_10k_2013.txt 17.2088261149 19.4673717189\n",
      "Data/EBAY_10k_2014.txt 17.522305957 19.844332095\n",
      "Data/EBAY_10k_2015.txt 17.1741438469 19.5172704435\n",
      "Data/EBAY_10k_2016.txt 16.8119978036 19.2121925858\n",
      "Data/EBAY_10k_2017.txt 16.988036714 19.3980211714\n",
      "Data/SHLDQ_10k_2013.txt 16.8126305116 19.2154420317\n",
      "Data/SHLDQ_10k_2014.txt 17.1138126995 19.5253765922\n",
      "Data/SHLDQ_10k_2015.txt 18.304118527 21.0016011567\n",
      "Data/SHLDQ_10k_2016.txt 18.7321020854 21.4781606764\n",
      "Data/SHLDQ_10k_2017.txt 17.755571973 20.6452057848\n"
     ]
    }
   ],
   "source": [
    "filelist_10k=!ls Data/*10k*txt\n",
    "\n",
    "\n",
    "flesch = []\n",
    "fog = []\n",
    "\n",
    "for file in filelist_10k:\n",
    "    with open(file, 'r') as f:\n",
    "        text=''.join(f).replace(';','.')\n",
    "        flesch.append(flesch_index(text))\n",
    "        fog.append(fog_index(text))\n",
    "        print(file, flesch[-1],fog[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superficially, and according to our readability metrics, reading 10-Ks is harder than reading articles on theoretical physics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
